"""
==========================================================
í™˜ê²½ ë° ì„¤ì¹˜ ì•ˆë‚´ (Python 3.10 ê¸°ì¤€)
==========================================================

1ï¸âƒ£ Python ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
> conda create -n capstone python=3.10
> conda activate capstone

2ï¸âƒ£ í•„ìš”í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
> pip install faster-whisper pydub numpy tqdm kospellpy
> pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

3ï¸âƒ£ ffmpeg ì„¤ì¹˜ (Windows)
> choco install ffmpeg
> ffmpeg -version

==========================================================
ğŸ’¡ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì ˆ ì•ˆë‚´ (faster-whisper)
==========================================================

1. ëª¨ë¸ ì‚¬ì´ì¦ˆ (MODEL_SIZE)
- "tiny", "base", "small", "medium", "large-v2", "large-v3" ë“± ì„ íƒ ê°€ëŠ¥
- í¬ê¸°ê°€ í´ìˆ˜ë¡ ì •í™•ë„ â†‘, ì†ë„ â†“, VRAM â†‘
- ì˜ˆ: MODEL_SIZE="large-v3"

2. Beam search í¬ê¸° (BEAM_SIZE)
- ë””ì½”ë”© ì‹œ íƒìƒ‰ ê¹Šì´ë¥¼ ì¡°ì ˆ
- ê°’ì´ í´ìˆ˜ë¡ ì •í™•ë„ â†‘, ì†ë„ â†“
- ì˜ˆ: BEAM_SIZE=5

3. ì˜¤ë²„ë© ì‹œê°„ (OVERLAP_SEC)
- VAD ê¸°ë°˜ ë¶„í•  ì‹œ ì¸ì ‘ êµ¬ê°„ ì¤‘ì²© ì‹œê°„
- ìŒì„± êµ¬ê°„ ì˜ë¦¼ ë°©ì§€ìš©
- ì˜ˆ: OVERLAP_SEC=2 (ì´ˆ)

4. ë§ì¶¤ë²• ê²€ì‚¬ ë°°ì¹˜ í¬ê¸° (BATCH_SENTENCE_COUNT)
- í•œ ë²ˆì— ë§ì¶¤ë²• ê²€ì‚¬í•  ë¬¸ì¥ ìˆ˜
- ê°’ì´ í´ìˆ˜ë¡ ì²˜ë¦¬ íš¨ìœ¨ â†‘, ë©”ëª¨ë¦¬ ì‚¬ìš© â†‘
- ì˜ˆ: BATCH_SENTENCE_COUNT=20

==========================================================
ğŸ’¡ ì‹¤í–‰ ì˜ˆì‹œ
==========================================================

from refine_transcription import refine_transcription
from faster_whisper import WhisperModel
import torch
import os

# --- ëª¨ë¸ í•œ ë²ˆë§Œ ë¡œë“œ ---
device = "cuda" if torch.cuda.is_available() else "cpu"
compute_type = "float16" if device == "cuda" else "int8"
model = WhisperModel("large-v3", device=device, compute_type=compute_type)

# --- ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ---
INPUT_MP3_FILE = "./ìˆ˜ì‚¬ê¸°ê´€ ì‚¬ì¹­í˜•(ê²€ì°°, ê²½ì°° ë“±)/63.mp3"
OUTPUT_FILE = os.path.splitext(INPUT_MP3_FILE)[0] + "_refined.txt"

sentence_count = refine_transcription(
    input_file=INPUT_MP3_FILE,
    output_file=OUTPUT_FILE,
    model=model,
    beam_size=5,          # í•„ìš” ì‹œ ì¡°ì •
    overlap_sec=2,        # í•„ìš” ì‹œ ì¡°ì •
    batch_sentence_count=20  # í•„ìš” ì‹œ ì¡°ì •
)

print(f"ì´ ë¬¸ì¥ ìˆ˜: {sentence_count}")
==========================================================
ğŸ’¡ í´ë” ë‚´ ëª¨ë“  MP3 ì²˜ë¦¬ ì˜ˆì‹œ
==========================================================

# process_mp3_files.py ì°¸ê³ 
# - INPUT_DIR í´ë” ë‚´ ëª¨ë“  MP3 íŒŒì¼ì„ ìˆ«ì ìˆœìœ¼ë¡œ ì²˜ë¦¬
# - ì¤‘ë³µ ì œê±° + ì§§ì€ ë¬¸ì¥ ì œê±° + ë§ì¶¤ë²• êµì •
# - output TXT íŒŒì¼ê³¼ metadata.json ìƒì„±

==========================================================
"""


import os
import json
import torch
from refine_transcription import refine_transcription
from faster_whisper import WhisperModel

# -------------------- ì„¤ì • --------------------
INPUT_DIR = "./ìˆ˜ì‚¬ê¸°ê´€ ì‚¬ì¹­í˜•(ê²€ì°°, ê²½ì°° ë“±)"
OUTPUT_DIR = "./ëŒ€í™” ë°ì´í„°"
MODEL_SIZE = "large-v3"
BEAM_SIZE = 5
OVERLAP_SEC = 3
BATCH_SENTENCE_COUNT = 40

# -------------------- Whisper ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ) --------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
compute_type = "float16" if device == "cuda" else "int8"
print(f"Using device: {device}, compute_type={compute_type}")
print(f"Loading faster-whisper model '{MODEL_SIZE}'...")
model = WhisperModel(MODEL_SIZE, device=device, compute_type=compute_type)
print("Whisper model loaded.")

# -------------------- MP3 íŒŒì¼ ìˆ«ì ìˆœ ì •ë ¬ --------------------
file_list = sorted(
    [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(".mp3")],
    key=lambda x: int(os.path.splitext(x)[0])
)

os.makedirs(OUTPUT_DIR, exist_ok=True)
META_FILE = os.path.join(OUTPUT_DIR, "metadata.json")
all_meta = {}
total_sentences = 0

# -------------------- ë°˜ë³µ ì²˜ë¦¬ --------------------
for filename in file_list:
    input_path = os.path.join(INPUT_DIR, filename)
    output_filename = os.path.splitext(filename)[0] + "_refined.txt"
    output_path = os.path.join(OUTPUT_DIR, output_filename)

    print(f"\nProcessing file: {filename}")
    sentence_count = refine_transcription(
        input_file=input_path,
        output_file=output_path,
        model=model,
        beam_size=BEAM_SIZE,
        overlap_sec=OVERLAP_SEC,
        batch_sentence_count=BATCH_SENTENCE_COUNT
    )

    all_meta[filename] = {
        "sentence_count": sentence_count,
        "output_file": output_path
    }
    total_sentences += sentence_count

all_meta["total_sentence_count"] = total_sentences

# -------------------- ë©”íƒ€ë°ì´í„° ì €ì¥ --------------------
with open(META_FILE, "w", encoding="utf-8") as f:
    json.dump(all_meta, f, ensure_ascii=False, indent=2)

print(f"\nAll files processed. Metadata saved to: {META_FILE}")
